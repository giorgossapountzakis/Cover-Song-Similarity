{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring single file structure:\n",
      "Examining first work folder: W_1002_crema\n",
      "Examining file: P_1002_crema.h5\n",
      "\n",
      "File structure:\n",
      "==================================================\n",
      "Top-level keys: ['crema']\n",
      "\n",
      "Dataset: crema\n",
      "  Shape: (19492, 12)\n",
      "  Type: float32\n",
      "  Attributes: ['CLASS', 'TITLE', 'VERSION']\n",
      "  First few values shape: (5, 5)\n",
      "\n",
      "Validating multiple files:\n",
      "\n",
      "Validating multiple files...\n",
      "==================================================\n",
      "\n",
      "File: P_1002_crema.h5\n",
      "Keys: ['crema']\n",
      "Shapes: {'crema': (19492, 12)}\n",
      "\n",
      "File: P_122525_crema.h5\n",
      "Keys: ['crema']\n",
      "Shapes: {'crema': (17404, 12)}\n",
      "\n",
      "File: P_129091_crema.h5\n",
      "Keys: ['crema']\n",
      "Shapes: {'crema': (18801, 12)}\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from pathlib import Path\n",
    "\n",
    "def explore_h5_structure(base_folder):\n",
    "    \"\"\"\n",
    "    Explore the structure of H5 files in the dataset\n",
    "    \n",
    "    This function will:\n",
    "    1. Find the first H5 file in the dataset\n",
    "    2. Print its complete internal structure\n",
    "    3. Show sample data shapes and types\n",
    "    \"\"\"\n",
    "    # Convert to Path object if it's a string\n",
    "    base_path = Path(base_folder)\n",
    "    \n",
    "    # Get the first work folder\n",
    "    first_work = next(base_path.iterdir())\n",
    "    print(f\"Examining first work folder: {first_work.name}\")\n",
    "    \n",
    "    # Get the first H5 file\n",
    "    first_file = next(first_work.glob(\"*.h5\"))\n",
    "    print(f\"Examining file: {first_file.name}\")\n",
    "    \n",
    "    def print_h5_structure(name, item):\n",
    "        \"\"\"Helper function to recursively print H5 structure\"\"\"\n",
    "        if isinstance(item, h5py.Dataset):\n",
    "            print(f\"\\nDataset: {name}\")\n",
    "            print(f\"  Shape: {item.shape}\")\n",
    "            print(f\"  Type: {item.dtype}\")\n",
    "            print(f\"  Attributes: {list(item.attrs.keys())}\")\n",
    "            \n",
    "            # Print a small sample of data\n",
    "            try:\n",
    "                if len(item.shape) == 2:\n",
    "                    print(f\"  First few values shape: {item[:5, :5].shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Cannot read sample: {e}\")\n",
    "        elif isinstance(item, h5py.Group):\n",
    "            print(f\"\\nGroup: {name}\")\n",
    "            print(f\"  Attributes: {list(item.attrs.keys())}\")\n",
    "    \n",
    "    try:\n",
    "        with h5py.File(first_file, 'r') as f:\n",
    "            print(\"\\nFile structure:\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"Top-level keys: {list(f.keys())}\")\n",
    "            f.visititems(print_h5_structure)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "\n",
    "# Let's also create a function to validate multiple files\n",
    "def validate_multiple_files(base_folder, num_files=3):\n",
    "    \"\"\"\n",
    "    Check multiple H5 files to ensure consistent structure\n",
    "    \"\"\"\n",
    "    base_path = Path(base_folder)\n",
    "    structures = []\n",
    "    \n",
    "    print(\"\\nValidating multiple files...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for work_folder in base_path.iterdir():\n",
    "        if len(structures) >= num_files:\n",
    "            break\n",
    "            \n",
    "        for h5_file in work_folder.glob(\"*.h5\"):\n",
    "            if len(structures) >= num_files:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                with h5py.File(h5_file, 'r') as f:\n",
    "                    structure = {\n",
    "                        'file': h5_file.name,\n",
    "                        'keys': list(f.keys()),\n",
    "                        'shapes': {k: f[k].shape for k in f.keys() if isinstance(f[k], h5py.Dataset)}\n",
    "                    }\n",
    "                    structures.append(structure)\n",
    "                    print(f\"\\nFile: {h5_file.name}\")\n",
    "                    print(f\"Keys: {structure['keys']}\")\n",
    "                    print(f\"Shapes: {structure['shapes']}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error with file {h5_file}: {e}\")\n",
    "    \n",
    "    return structures\n",
    "\n",
    "\n",
    "base_folder = r'D:\\TACOS\\da-tacos_benchmark_subset_crema\\da-tacos_benchmark_subset_crema'\n",
    "print(\"Exploring single file structure:\")\n",
    "explore_h5_structure(base_folder)\n",
    "\n",
    "print(\"\\nValidating multiple files:\")\n",
    "structures = validate_multiple_files(base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class MemoryEfficientDTW:\n",
    "    def __init__(self, window_size=None, target_length=2000):\n",
    "        \"\"\"\n",
    "        Initialize the memory-efficient DTW comparator\n",
    "        \n",
    "        Parameters:\n",
    "        window_size (int): Size of the Sakoe-Chiba window for constrained DTW\n",
    "        target_length (int): Target length after downsampling\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def load_features(self, h5_path):\n",
    "        \"\"\"\n",
    "        Load and preprocess chromagram features efficiently\n",
    "        \"\"\"\n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            # Load features as float32 instead of float64 to save memory\n",
    "            features = np.array(f['crema'], dtype=np.float32)\n",
    "            \n",
    "            # Normalize features efficiently\n",
    "            row_sums = np.sum(features, axis=1, keepdims=True)\n",
    "            features = np.divide(features, row_sums, where=row_sums != 0)\n",
    "            \n",
    "            return features\n",
    "\n",
    "    def downsample_sequence(self, sequence):\n",
    "        \"\"\"\n",
    "        Downsample a sequence to reduce memory usage\n",
    "        \"\"\"\n",
    "        # Calculate downsample factor based on sequence length\n",
    "        downsample_factor = max(1, len(sequence) // self.target_length)\n",
    "        return sequence[::downsample_factor]\n",
    "\n",
    "    def compute_dtw_distance(self, seq1, seq2):\n",
    "        \"\"\"\n",
    "        Compute DTW distance with memory efficiency optimizations\n",
    "        \"\"\"\n",
    "        # Convert to float32 for memory efficiency\n",
    "        seq1 = seq1.astype(np.float32)\n",
    "        seq2 = seq2.astype(np.float32)\n",
    "        \n",
    "        # Downsample sequences\n",
    "        seq1_ds = self.downsample_sequence(seq1)\n",
    "        seq2_ds = self.downsample_sequence(seq2)\n",
    "        \n",
    "        N, M = len(seq1_ds), len(seq2_ds)\n",
    "        \n",
    "        # Use two rows instead of full matrix\n",
    "        previous_row = np.full(M, np.inf, dtype=np.float32)\n",
    "        current_row = np.full(M, np.inf, dtype=np.float32)\n",
    "        \n",
    "        # Initialize first cell\n",
    "        current_row[0] = cdist(\n",
    "            seq1_ds[0].reshape(1, -1), \n",
    "            seq2_ds[0].reshape(1, -1)\n",
    "        )[0, 0]\n",
    "        \n",
    "        # Fill first row\n",
    "        for j in range(1, M):\n",
    "            current_row[j] = current_row[j-1] + cdist(\n",
    "                seq1_ds[0].reshape(1, -1),\n",
    "                seq2_ds[j].reshape(1, -1)\n",
    "            )[0, 0]\n",
    "        \n",
    "        # Process rest of the sequences\n",
    "        for i in range(1, N):\n",
    "            # Swap rows\n",
    "            previous_row, current_row = current_row, previous_row\n",
    "            \n",
    "            # Calculate window boundaries\n",
    "            if self.window_size:\n",
    "                j_start = max(0, i - self.window_size)\n",
    "                j_end = min(M, i + self.window_size + 1)\n",
    "            else:\n",
    "                j_start, j_end = 0, M\n",
    "            \n",
    "            # Initialize first column\n",
    "            current_row[0] = previous_row[0] + cdist(\n",
    "                seq1_ds[i].reshape(1, -1),\n",
    "                seq2_ds[0].reshape(1, -1)\n",
    "            )[0, 0]\n",
    "            \n",
    "            # Fill current row\n",
    "            for j in range(max(1, j_start), j_end):\n",
    "                cost = cdist(\n",
    "                    seq1_ds[i].reshape(1, -1),\n",
    "                    seq2_ds[j].reshape(1, -1)\n",
    "                )[0, 0]\n",
    "                \n",
    "                current_row[j] = cost + min(\n",
    "                    previous_row[j],     # vertical\n",
    "                    current_row[j-1],    # horizontal\n",
    "                    previous_row[j-1]    # diagonal\n",
    "                )\n",
    "        \n",
    "        # Return normalized distance\n",
    "        return current_row[-1] / (N + M)\n",
    "\n",
    "    def compare_performances(self, perf1_path, perf2_path):\n",
    "        \"\"\"\n",
    "        Compare two performances with key-invariance\n",
    "        \"\"\"\n",
    "        # Load features\n",
    "        chroma1 = self.load_features(perf1_path)\n",
    "        chroma2 = self.load_features(perf2_path)\n",
    "        \n",
    "        # Try all possible key shifts\n",
    "        min_distance = float('inf')\n",
    "        best_shift = 0\n",
    "        \n",
    "        for shift in range(12):\n",
    "            # Shift second chromagram\n",
    "            shifted_chroma2 = np.roll(chroma2, shift, axis=1)\n",
    "            \n",
    "            # Compute DTW distance\n",
    "            distance = self.compute_dtw_distance(chroma1, shifted_chroma2)\n",
    "            \n",
    "            # Update if this is the best match\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                best_shift = shift\n",
    "        \n",
    "        # Convert distance to similarity score\n",
    "        similarity = 1 / (1 + min_distance)\n",
    "        \n",
    "        return similarity, best_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "class MemoryEfficientDatasetProcessor:\n",
    "    def __init__(self, base_folder, dtw_comparator, max_files_per_work=5):\n",
    "        \"\"\"\n",
    "        Initialize the dataset processor with memory efficiency in mind\n",
    "        \n",
    "        Parameters:\n",
    "        base_folder (str): Path to the main folder containing work subfolders\n",
    "        dtw_comparator: Instance of MemoryEfficientDTW\n",
    "        max_files_per_work (int): Maximum number of files to process per work\n",
    "        \"\"\"\n",
    "        self.base_folder = Path(base_folder)\n",
    "        self.comparator = dtw_comparator\n",
    "        self.max_files_per_work = max_files_per_work\n",
    "        self.works = self._get_works()\n",
    "    \n",
    "    def _get_works(self):\n",
    "        \"\"\"Get all work folders in the dataset\"\"\"\n",
    "        return [d for d in self.base_folder.iterdir() if d.is_dir()]\n",
    "    \n",
    "    def _get_performances(self, work_path):\n",
    "        \"\"\"Get a subset of h5 files for a specific work\"\"\"\n",
    "        all_performances = list(work_path.glob(\"*.h5\"))\n",
    "        if len(all_performances) > self.max_files_per_work:\n",
    "            return np.random.choice(all_performances, \n",
    "                                  self.max_files_per_work, \n",
    "                                  replace=False).tolist()\n",
    "        return all_performances\n",
    "    \n",
    "    def evaluate_single_work(self, work_path):\n",
    "        \"\"\"Evaluate a subset of pairwise comparisons within a single work\"\"\"\n",
    "        performances = self._get_performances(work_path)\n",
    "        scores = []\n",
    "        \n",
    "        print(f\"Processing work: {work_path.name}\")\n",
    "        for perf1, perf2 in tqdm(list(combinations(performances, 2)), \n",
    "                                desc=\"Comparing performances\"):\n",
    "            try:\n",
    "                similarity, shift = self.comparator.compare_performances(\n",
    "                    str(perf1), str(perf2)\n",
    "                )\n",
    "                scores.append({\n",
    "                    'work': work_path.name,\n",
    "                    'perf1': perf1.name,\n",
    "                    'perf2': perf2.name,\n",
    "                    'similarity': similarity,\n",
    "                    'key_shift': shift,\n",
    "                    'same_work': True\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {perf1.name} and {perf2.name}: {str(e)}\")\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def evaluate_between_works(self, work1_path, work2_path, num_samples=2):\n",
    "        \"\"\"Compare a small sample of performances between works\"\"\"\n",
    "        perfs1 = self._get_performances(work1_path)[:num_samples]\n",
    "        perfs2 = self._get_performances(work2_path)[:num_samples]\n",
    "        \n",
    "        scores = []\n",
    "        print(f\"Comparing {work1_path.name} with {work2_path.name}\")\n",
    "        for perf1 in perfs1:\n",
    "            for perf2 in perfs2:\n",
    "                try:\n",
    "                    similarity, shift = self.comparator.compare_performances(\n",
    "                        str(perf1), str(perf2)\n",
    "                    )\n",
    "                    scores.append({\n",
    "                        'work1': work1_path.name,\n",
    "                        'work2': work2_path.name,\n",
    "                        'perf1': perf1.name,\n",
    "                        'perf2': perf2.name,\n",
    "                        'similarity': similarity,\n",
    "                        'key_shift': shift,\n",
    "                        'same_work': False\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error comparing {perf1.name} and {perf2.name}: {str(e)}\")\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def analyze_dataset(self, num_works=None, num_between_samples=2):\n",
    "        \"\"\"\n",
    "        Analyze a subset of the dataset with memory efficiency in mind\n",
    "        \n",
    "        Parameters:\n",
    "        num_works: Number of works to analyze (None for all)\n",
    "        num_between_samples: Number of samples for between-work comparisons\n",
    "        \"\"\"\n",
    "        all_scores = []\n",
    "        \n",
    "        # Select works to analyze\n",
    "        selected_works = self.works\n",
    "        if num_works is not None:\n",
    "            selected_works = np.random.choice(self.works, \n",
    "                                            min(num_works, len(self.works)), \n",
    "                                            replace=False)\n",
    "        \n",
    "        # Within-work comparisons\n",
    "        print(\"Analyzing within-work comparisons...\")\n",
    "        for work in selected_works:\n",
    "            scores = self.evaluate_single_work(work)\n",
    "            all_scores.extend(scores)\n",
    "            \n",
    "            # Clear memory periodically\n",
    "            if len(all_scores) > 1000:\n",
    "                temp_df = pd.DataFrame(all_scores)\n",
    "                all_scores = [temp_df]\n",
    "        \n",
    "        # Between-work comparisons\n",
    "        print(\"Analyzing between-work comparisons...\")\n",
    "        for work1, work2 in combinations(selected_works, 2):\n",
    "            scores = self.evaluate_between_works(\n",
    "                work1, work2, num_between_samples\n",
    "            )\n",
    "            all_scores.extend(scores)\n",
    "        \n",
    "        # Combine all results\n",
    "        if isinstance(all_scores[0], pd.DataFrame):\n",
    "            final_df = pd.concat(all_scores)\n",
    "        else:\n",
    "            final_df = pd.DataFrame(all_scores)\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    def plot_similarity_distribution(self, results_df, save_path=None):\n",
    "        \"\"\"Plot similarity score distribution with memory efficiency\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Process data in chunks if necessary\n",
    "        chunk_size = 1000\n",
    "        for chunk in np.array_split(results_df, max(1, len(results_df) // chunk_size)):\n",
    "            sns.histplot(data=chunk, x='similarity', hue='same_work', \n",
    "                        bins=30, alpha=0.6, stat='density')\n",
    "        \n",
    "        plt.title('Distribution of Similarity Scores')\n",
    "        plt.xlabel('Similarity Score')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend(['Different Works', 'Same Work'])\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "    \n",
    "    def compute_metrics(self, results_df, threshold=0.7):\n",
    "        \"\"\"Compute evaluation metrics with memory efficiency\"\"\"\n",
    "        # Process in chunks if the dataset is large\n",
    "        chunk_size = 1000\n",
    "        metrics = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "        \n",
    "        for chunk in np.array_split(results_df, max(1, len(results_df) // chunk_size)):\n",
    "            predictions = chunk['similarity'] >= threshold\n",
    "            true_labels = chunk['same_work']\n",
    "            \n",
    "            metrics['tp'] += sum(predictions & true_labels)\n",
    "            metrics['fp'] += sum(predictions & ~true_labels)\n",
    "            metrics['tn'] += sum(~predictions & ~true_labels)\n",
    "            metrics['fn'] += sum(~predictions & true_labels)\n",
    "        \n",
    "        precision = metrics['tp'] / (metrics['tp'] + metrics['fp']) if (metrics['tp'] + metrics['fp']) > 0 else 0\n",
    "        recall = metrics['tp'] / (metrics['tp'] + metrics['fn']) if (metrics['tp'] + metrics['fn']) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'threshold': threshold,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            **metrics\n",
    "        }\n",
    "\n",
    "    def plot_chromagram(self, file_path, downsample_factor=200, save_path=None):\n",
    "        \"\"\"Plot chromagram with aggressive downsampling for visualization\"\"\"\n",
    "        features = self.comparator.load_features(str(file_path))\n",
    "        features = features[::downsample_factor]\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.imshow(features.T, aspect='auto', origin='lower', \n",
    "                  interpolation='nearest', cmap='Blues')\n",
    "        plt.colorbar(label='Magnitude')\n",
    "        plt.ylabel('Pitch Class')\n",
    "        plt.xlabel(f'Time (downsampled by factor of {downsample_factor})')\n",
    "        plt.title(f'Chromagram: {Path(file_path).name}')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize with modest parameters for testing\n",
    "# dtw_comparator = MusicDTWComparator(window_size=200)  # Larger window since we're downsampling\n",
    "# processor = MusicDatasetProcessor(r'D:\\TACOS\\da-tacos_benchmark_subset_crema\\da-tacos_benchmark_subset_crema', dtw_comparator)\n",
    "\n",
    "# # Start with a small subset first\n",
    "# results = processor.analyze_dataset(num_between_samples=2)  # Small number for testing\n",
    "\n",
    "# # Create some visualizations\n",
    "# processor.plot_similarity_distribution(results)\n",
    "# plt.show()\n",
    "\n",
    "# # Find the optimal threshold\n",
    "# best_metrics = processor.find_optimal_threshold(results)\n",
    "# print(f\"Optimal threshold: {best_metrics['threshold']}\")\n",
    "# print(f\"F1 Score: {best_metrics['f1_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis...\n",
      "Processing selected works...\n",
      "Analyzing within-work comparisons...\n",
      "Processing work: W_5283_crema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comparing performances: 100%|██████████| 10/10 [01:32<00:00,  9.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing work: W_21826_crema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comparing performances: 100%|██████████| 10/10 [01:28<00:00,  8.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing work: W_30309_crema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comparing performances: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing between-work comparisons...\n",
      "Comparing W_5283_crema with W_21826_crema\n",
      "Comparing W_5283_crema with W_30309_crema\n",
      "Comparing W_21826_crema with W_30309_crema\n",
      "\n",
      "Generating visualizations...\n",
      "Plotting chromagram...\n",
      "Plotting similarity distribution...\n",
      "Saving similarity data...\n",
      "Computing metrics...\n",
      "\n",
      "Summary Statistics:\n",
      "--------------------------------------------------\n",
      "Number of works analyzed: 4\n",
      "Total comparisons made: 28\n",
      "Average similarity score: 0.825\n",
      "\n",
      "Unique works analyzed:\n",
      "Error during analysis: '<' not supported between instances of 'str' and 'float'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m croma_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTACOS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mda-tacos_benchmark_subset_crema\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mda-tacos_benchmark_subset_crema\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Run analysis with conservative parameters\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m results, metrics \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_dataset_subset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcroma_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_works\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_files_per_work\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[0;32m    122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 103\u001b[0m, in \u001b[0;36manalyze_dataset_subset\u001b[1;34m(croma_folder, num_works, max_files_per_work)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage similarity score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUnique works analyzed:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m work \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munique_works\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwork\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, metrics\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'float'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_dataset_subset(croma_folder, num_works=3, max_files_per_work=5):\n",
    "    \"\"\"\n",
    "    Analyze a subset of the dataset with memory-efficient processing and visualization\n",
    "    \n",
    "    Parameters:\n",
    "    croma_folder: Path to the main folder containing work subfolders\n",
    "    num_works: Number of works to analyze\n",
    "    max_files_per_work: Maximum number of files to process per work\n",
    "    \"\"\"\n",
    "    # Initialize our memory-efficient DTW comparator\n",
    "    dtw_comparator = MemoryEfficientDTW(\n",
    "        window_size=50,\n",
    "        target_length=1000\n",
    "    )\n",
    "    \n",
    "    # Initialize the memory-efficient dataset processor\n",
    "    processor = MemoryEfficientDatasetProcessor(\n",
    "        croma_folder,\n",
    "        dtw_comparator,\n",
    "        max_files_per_work=max_files_per_work\n",
    "    )\n",
    "    \n",
    "    # Select works to analyze\n",
    "    works_list = list(Path(croma_folder).glob(\"W_*\"))\n",
    "    if not works_list:\n",
    "        raise ValueError(f\"No work folders found in {croma_folder}\")\n",
    "    \n",
    "    selected_works = np.random.choice(\n",
    "        works_list,\n",
    "        min(num_works, len(works_list)),\n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    # Create visualization directory\n",
    "    viz_folder = Path(\"visualization_results\")\n",
    "    viz_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"Starting analysis...\")\n",
    "    try:\n",
    "        # Analyze the dataset\n",
    "        print(\"Processing selected works...\")\n",
    "        results = processor.analyze_dataset(\n",
    "            num_works=len(selected_works),\n",
    "            num_between_samples=2\n",
    "        )\n",
    "        \n",
    "        # Create visualizations\n",
    "        print(\"\\nGenerating visualizations...\")\n",
    "        \n",
    "        # 1. Plot example chromagram\n",
    "        print(\"Plotting chromagram...\")\n",
    "        sample_file = next(selected_works[0].glob(\"*.h5\"))\n",
    "        processor.plot_chromagram(\n",
    "            sample_file,\n",
    "            downsample_factor=200,\n",
    "            save_path=viz_folder / 'chromagram_example.png'\n",
    "        )\n",
    "        \n",
    "        # 2. Plot similarity distribution\n",
    "        print(\"Plotting similarity distribution...\")\n",
    "        processor.plot_similarity_distribution(\n",
    "            results,\n",
    "            save_path=viz_folder / 'similarity_distribution.png'\n",
    "        )\n",
    "        \n",
    "        # 3. Save raw similarity scores\n",
    "        print(\"Saving similarity data...\")\n",
    "        results.to_csv(viz_folder / 'similarity_scores.csv', index=False)\n",
    "        \n",
    "        # 4. Compute and save metrics\n",
    "        print(\"Computing metrics...\")\n",
    "        metrics = processor.compute_metrics(results, threshold=0.7)\n",
    "        \n",
    "        # Save metrics to file\n",
    "        with open(viz_folder / 'analysis_metrics.txt', 'w') as f:\n",
    "            f.write(\"Analysis Metrics:\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\")\n",
    "            for key, value in metrics.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    f.write(f\"{key}: {value:.3f}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        # Print summary statistics correctly based on DataFrame structure\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Get unique works from both work1 and work2 columns\n",
    "        unique_works = pd.concat([\n",
    "            results['work1'] if 'work1' in results.columns else pd.Series(),\n",
    "            results['work2'] if 'work2' in results.columns else pd.Series()\n",
    "        ]).unique()\n",
    "        \n",
    "        print(f\"Number of works analyzed: {len(unique_works)}\")\n",
    "        print(f\"Total comparisons made: {len(results)}\")\n",
    "        print(f\"Average similarity score: {results['similarity'].mean():.3f}\")\n",
    "        print(\"\\nUnique works analyzed:\")\n",
    "        \n",
    "        return results, metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the path to your dataset\n",
    "    croma_folder = r'D:\\TACOS\\da-tacos_benchmark_subset_crema\\da-tacos_benchmark_subset_crema'\n",
    "    \n",
    "    # Run analysis with conservative parameters\n",
    "    results, metrics = analyze_dataset_subset(\n",
    "        croma_folder,\n",
    "        num_works=3,\n",
    "        max_files_per_work=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
