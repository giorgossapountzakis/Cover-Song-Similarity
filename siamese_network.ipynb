{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading and preprocessing chroma features from H5 files.\n",
    "\n",
    "    This dataset handles the loading of features, downsampling, and normalization\n",
    "    of the chromagrams for the Siamese network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pairs_list, target_length=2000):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            pairs_list: List of tuples (file_path1, file_path2, label)\n",
    "            target_length: Target sequence length after downsampling\n",
    "        \"\"\"\n",
    "        self.pairs_list = pairs_list\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs_list)\n",
    "\n",
    "    def load_features(self, h5_path):\n",
    "        \"\"\"\n",
    "        Load chromagram features from H5 file and preprocess them.\n",
    "\n",
    "        Args:\n",
    "            h5_path: Path to the H5 file\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed chromagram features as a tensor\n",
    "        \"\"\"\n",
    "        with h5py.File(h5_path, \"r\") as f:\n",
    "            # Load the chroma/HPCP features - adjust key based on your file structure\n",
    "            features = np.array(f[\"crema\"], dtype=np.float32)\n",
    "\n",
    "            # Normalize features\n",
    "            row_norms = np.sqrt(np.sum(features**2, axis=1, keepdims=True))\n",
    "            features = np.divide(features, row_norms, where=row_norms > 1e-10)\n",
    "\n",
    "            # Downsample if needed\n",
    "            if len(features) > self.target_length:\n",
    "                # Calculate downsample factor based on sequence length\n",
    "                downsample_factor = len(features) // self.target_length\n",
    "                features = features[::downsample_factor]\n",
    "\n",
    "            # Trim or pad to target_length\n",
    "            if len(features) > self.target_length:\n",
    "                features = features[: self.target_length]\n",
    "            elif len(features) < self.target_length:\n",
    "                pad_width = ((0, self.target_length - len(features)), (0, 0))\n",
    "                features = np.pad(features, pad_width, mode=\"constant\")\n",
    "\n",
    "            # Convert to torch tensor\n",
    "            return torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path1, path2, label = self.pairs_list[idx]\n",
    "\n",
    "        # Load and preprocess features\n",
    "        features1 = self.load_features(path1)\n",
    "        features2 = self.load_features(path2)\n",
    "\n",
    "        return {\"chroma1\": features1, \"chroma2\": features2, \"label\": torch.tensor(label, dtype=torch.float32), \"path1\": path1, \"path2\": path2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(base_folder, num_positive=1000, num_negative=1000, max_files_per_work=5):\n",
    "    \"\"\"\n",
    "    Generate pairs of songs for training the Siamese network.\n",
    "\n",
    "    Args:\n",
    "        base_folder: Base folder containing works and their chromagrams\n",
    "        num_positive: Number of positive pairs to generate\n",
    "        num_negative: Number of negative pairs to generate\n",
    "        max_files_per_work: Maximum number of files to consider per work\n",
    "\n",
    "    Returns:\n",
    "        List of tuples (file_path1, file_path2, label)\n",
    "    \"\"\"\n",
    "    base_path = Path(base_folder)\n",
    "    work_folders = [d for d in base_path.iterdir() if d.is_dir()]\n",
    "    pairs = []\n",
    "\n",
    "    # Generate positive pairs (same work)\n",
    "    positive_count = 0\n",
    "    print(\"Generating positive pairs...\")\n",
    "\n",
    "    for work_folder in tqdm(work_folders):\n",
    "        if positive_count >= num_positive:\n",
    "            break\n",
    "\n",
    "        # Get all H5 files for this work\n",
    "        h5_files = list(work_folder.glob(\"*.h5\"))\n",
    "\n",
    "        # If we have too many files, sample a subset\n",
    "        if len(h5_files) > max_files_per_work:\n",
    "            h5_files = random.sample(h5_files, max_files_per_work)\n",
    "\n",
    "        # Skip if we have fewer than 2 files\n",
    "        if len(h5_files) < 2:\n",
    "            continue\n",
    "\n",
    "        # Generate pairs\n",
    "        for i in range(len(h5_files)):\n",
    "            for j in range(i + 1, len(h5_files)):\n",
    "                pairs.append((str(h5_files[i]), str(h5_files[j]), 1))  # Label 1 for positive pairs\n",
    "                positive_count += 1\n",
    "\n",
    "                if positive_count >= num_positive:\n",
    "                    break\n",
    "            if positive_count >= num_positive:\n",
    "                break\n",
    "\n",
    "    # Generate negative pairs (different works)\n",
    "    negative_count = 0\n",
    "    print(\"Generating negative pairs...\")\n",
    "\n",
    "    while negative_count < num_negative:\n",
    "        # Select two different works\n",
    "        work1, work2 = random.sample(work_folders, 2)\n",
    "\n",
    "        # Get H5 files\n",
    "        h5_files1 = list(work1.glob(\"*.h5\"))\n",
    "        h5_files2 = list(work2.glob(\"*.h5\"))\n",
    "\n",
    "        # Skip if either work has no files\n",
    "        if not h5_files1 or not h5_files2:\n",
    "            continue\n",
    "\n",
    "        # Sample one file from each work\n",
    "        file1 = random.choice(h5_files1)\n",
    "        file2 = random.choice(h5_files2)\n",
    "\n",
    "        pairs.append((str(file1), str(file2), 0))  # Label 0 for negative pairs\n",
    "        negative_count += 1\n",
    "\n",
    "        if negative_count % 100 == 0:\n",
    "            print(f\"Generated {negative_count} negative pairs\")\n",
    "\n",
    "    # Shuffle pairs\n",
    "    random.shuffle(pairs)\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional block with batch normalization and optional dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dropout=0.2):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature extractor network for processing chromagrams.\n",
    "\n",
    "    This network converts a chromagram (or HPCP) sequence into a fixed-length embedding\n",
    "    using a series of convolutional layers and global pooling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dims=12, embedding_size=128):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        # Note: PyTorch expects input as (batch_size, channels, sequence_length)\n",
    "        # So our input is (batch_size, 12, sequence_length)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ConvBlock(input_dims, 32),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
    "            ConvBlock(32, 64),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
    "            ConvBlock(64, 128),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
    "            ConvBlock(128, 256),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
    "        )\n",
    "\n",
    "        # Global pooling and fully connected layers\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(nn.Linear(256 * 2, 512), nn.ReLU(), nn.Dropout(0.3), nn.Linear(512, embedding_size))  # *2 because we concatenate avg and max pool\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x has shape (batch_size, sequence_length, features)\n",
    "        # We need to transpose to (batch_size, features, sequence_length)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # Apply convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "\n",
    "        # Apply global pooling\n",
    "        avg_pool = self.global_avg_pool(x).squeeze(-1)\n",
    "        max_pool = self.global_max_pool(x).squeeze(-1)\n",
    "\n",
    "        # Concatenate pooling results\n",
    "        x = torch.cat((avg_pool, max_pool), dim=1)\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        embedding = self.fc_layers(x)\n",
    "\n",
    "        # Normalize the embedding to unit length\n",
    "        embedding = F.normalize(embedding, p=2, dim=1)\n",
    "\n",
    "        return embedding\n",
    "\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super(SiameseNet, self).__init__()\n",
    "\n",
    "        # Feature extractor\n",
    "        self.feature_extractor = FeatureExtractor(embedding_size=embedding_size)\n",
    "\n",
    "        # Final FC layers - create directly on target device\n",
    "        self.final_fc = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Extract features\n",
    "        embedding1 = self.feature_extractor(x1)\n",
    "        embedding2 = self.feature_extractor(x2)\n",
    "\n",
    "        # Compute distance with numerical stability\n",
    "        squared_diff = (embedding1 - embedding2) ** 2\n",
    "        squared_distance = torch.sum(squared_diff, dim=1, keepdim=True) + 1e-8\n",
    "        distance = torch.sqrt(squared_distance)\n",
    "\n",
    "        # Apply final layers\n",
    "        similarity = self.final_fc(distance)\n",
    "\n",
    "        return similarity, embedding1, embedding2, distance\n",
    "\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function for Siamese networks.\n",
    "\n",
    "    This loss function encourages similar pairs to have small distances and\n",
    "    dissimilar pairs to have distances larger than a margin.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, distance, y):\n",
    "\n",
    "        # distance: euclidean distance between embeddings\n",
    "        # y: binary label (1 if same, 0 if different)\n",
    "\n",
    "        # Loss for similar pairs: distance^2\n",
    "        similar_loss = y * torch.pow(distance, 2)\n",
    "\n",
    "        # Loss for dissimilar pairs: max(0, margin - distance)^2\n",
    "        dissimilar_loss = (1 - y) * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2)\n",
    "\n",
    "        # Mean loss across the batch\n",
    "        loss = torch.mean(similar_loss + dissimilar_loss) / 2.0\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_function, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation data.\n",
    "\n",
    "    Args:\n",
    "        model: The Siamese network\n",
    "        data_loader: DataLoader for validation data\n",
    "        loss_function: Loss function\n",
    "        device: Device to evaluate on (GPU/CPU)\n",
    "\n",
    "    Returns:\n",
    "        Average loss, predictions, true labels, distances\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_distances = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Move data to device\n",
    "            chroma1 = batch[\"chroma1\"].to(device)\n",
    "            chroma2 = batch[\"chroma2\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            preds, _, _, distances = model(chroma1, chroma2)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(distances, labels.view(-1, 1))\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_distances.extend(distances.cpu().numpy())\n",
    "\n",
    "    return running_loss / len(data_loader), np.array(all_preds), np.array(all_labels), np.array(all_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization directory\n",
    "viz_folder = Path(\"visualization_results_siamese\")\n",
    "viz_folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(distances, labels, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Plot metrics for model evaluation.\n",
    "\n",
    "    Args:\n",
    "        distances: Array of distances\n",
    "        labels: Array of true labels\n",
    "        threshold: Distance threshold for classification\n",
    "    \"\"\"\n",
    "    # Flatten arrays\n",
    "    distances = distances.flatten()\n",
    "    labels = labels.flatten()\n",
    "\n",
    "    # Precision-Recall curve\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(labels, 1 - distances)\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, roc_thresholds = roc_curve(labels, 1 - distances)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot both curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "\n",
    "    # Precision-Recall curve\n",
    "    ax1.plot(recall, precision, lw=2)\n",
    "    ax1.set_xlabel(\"Recall\")\n",
    "    ax1.set_ylabel(\"Precision\")\n",
    "    ax1.set_title(f\"Precision-Recall Curve\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # ROC curve\n",
    "    ax2.plot(fpr, tpr, lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "    ax2.plot([0, 1], [0, 1], \"k--\", lw=2)\n",
    "    ax2.set_xlabel(\"False Positive Rate\")\n",
    "    ax2.set_ylabel(\"True Positive Rate\")\n",
    "    ax2.set_title(\"ROC Curve\")\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(viz_folder / \"training_metrics.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot distance distributions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(distances[labels == 1], bins=30, alpha=0.5, label=\"Same Work (Cover)\")\n",
    "    plt.hist(distances[labels == 0], bins=30, alpha=0.5, label=\"Different Works (Not Cover)\")\n",
    "    plt.axvline(x=threshold, color=\"r\", linestyle=\"--\", label=f\"Basic Threshold ({threshold:.2f})\")\n",
    "    plt.xlabel(\"Distance\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distribution of Distances on training set\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(viz_folder / \"training_distance_distribution.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(embedding_size=128, learning_rate=0.001, margin=1.0):\n",
    "    \"\"\"\n",
    "    Create and initialize the model, optimizer, and loss function.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, optimizer, loss_function)\n",
    "    \"\"\"\n",
    "    # Check if CUDA is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create model\n",
    "    model = SiameseNet(embedding_size=embedding_size)\n",
    "    # print(\"Model architecture:\")\n",
    "    # print(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Create loss function and optimizer\n",
    "    loss_function = ContrastiveLoss(margin=margin)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    return model, optimizer, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated training function\n",
    "def train_epoch(model, data_loader, optimizer, loss_function):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    processed_batches = 0\n",
    "\n",
    "    # Force garbage collection before training\n",
    "    import gc\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"GPU memory at start: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"Training\")):\n",
    "        # Check if batch is empty\n",
    "        if not batch:\n",
    "            print(\"Warning: Empty batch encountered, skipping\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Move data to device\n",
    "            chroma1 = batch[\"chroma1\"].to(device)\n",
    "            chroma2 = batch[\"chroma2\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad(set_to_none=True)  # More efficient than setting to zero\n",
    "\n",
    "            # Forward pass\n",
    "            similarity, embed1, embed2, distances = model(chroma1, chroma2)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(distances, labels.view(-1, 1))\n",
    "\n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"NaN loss detected in batch {batch_idx}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping to prevent explosion\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            processed_batches += 1\n",
    "\n",
    "            # # Report progress periodically\n",
    "            # if batch_idx % 10 == 0:\n",
    "            #     print(f\"  Batch {batch_idx}: loss = {loss.item():.4f}, GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "            # # Print parameter stats to verify learning is happening\n",
    "            # if batch_idx == 0 or batch_idx % 50 == 0:\n",
    "            #     for name, param in model.named_parameters():\n",
    "            #         if param.grad is not None:\n",
    "            #             grad_mag = param.grad.abs().mean().item()\n",
    "            #             param_mag = param.abs().mean().item()\n",
    "            #             print(f\"    {name}: param_mag={param_mag:.6f}, grad_mag={grad_mag:.6f}\")\n",
    "\n",
    "            # Explicitly delete tensors to free memory\n",
    "            del chroma1, chroma2, similarity, embed1, embed2, distances\n",
    "\n",
    "            # Periodically clear cache\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "    # Final cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if processed_batches == 0:\n",
    "        raise ValueError(\"No batches were processed during training!\")\n",
    "\n",
    "    return running_loss / processed_batches\n",
    "\n",
    "\n",
    "# Main training orchestration\n",
    "def train_model(\n",
    "    base_folder,\n",
    "    model_save_path=\"siamese_cover_detector.pth\",\n",
    "    num_epochs=30,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    embedding_size=128,\n",
    "    margin=1.0,\n",
    "    target_length=2000,\n",
    "    num_positive=1000,\n",
    "    num_negative=1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete training process for the Siamese network.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate and prepare data\n",
    "    print(\"Generating pairs...\")\n",
    "    pairs = generate_pairs(base_folder, num_positive=num_positive, num_negative=num_negative)\n",
    "    train_pairs, val_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f\"Training pairs: {len(train_pairs)}\")\n",
    "    print(f\"Validation pairs: {len(val_pairs)}\")\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = ChromaDataset(train_pairs, target_length=target_length)\n",
    "    val_dataset = ChromaDataset(val_pairs, target_length=target_length)\n",
    "\n",
    "    # Adjust dataloader settings for better GPU performance\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    # Setup model, optimizer, and loss function\n",
    "    model, optimizer, loss_function = setup_model(\n",
    "        embedding_size=embedding_size,\n",
    "        learning_rate=learning_rate,\n",
    "        margin=margin,\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "    # Training loop tracking\n",
    "    best_val_loss = float(\"inf\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Create directory for saving models if needed\n",
    "    os.makedirs(os.path.dirname(model_save_path) if os.path.dirname(model_save_path) else \".\", exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            print(f\"\\n{'='*20} Epoch {epoch}/{num_epochs} {'='*20}\")\n",
    "\n",
    "            # Train\n",
    "            print(\"Starting training phase...\")\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, loss_function)\n",
    "            train_losses.append(train_loss)\n",
    "            print(f\"Training completed for epoch {epoch}. Loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Evaluate\n",
    "            print(\"Starting validation phase...\")\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            all_distances = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "                    # Move data to device\n",
    "                    chroma1 = batch[\"chroma1\"].to(device)\n",
    "                    chroma2 = batch[\"chroma2\"].to(device)\n",
    "                    labels = batch[\"label\"].to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    preds, _, _, distances = model(chroma1, chroma2)\n",
    "\n",
    "                    # Compute loss\n",
    "                    loss = loss_function(distances, labels.view(-1, 1))\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    # Store predictions and labels\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    all_distances.extend(distances.cpu().numpy())\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            # Update learning rate\n",
    "            previous_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            scheduler.step(val_loss)\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            if previous_lr != current_lr:\n",
    "                print(f\"Learning rate changed from {previous_lr} to {current_lr}\")\n",
    "\n",
    "            print(f\"Epoch {epoch} - Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model_state_dict\": model.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"val_loss\": val_loss,\n",
    "                    },\n",
    "                    model_save_path,\n",
    "                )\n",
    "                print(f\"Model saved at epoch {epoch}\")\n",
    "\n",
    "        # Plot metrics for best model\n",
    "        plot_metrics(\"Training metrics\", np.array(all_distances), np.array(all_labels))\n",
    "        print(f\"Metrics plotted for epoch {epoch}\")\n",
    "\n",
    "        # Plot training curves\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\")\n",
    "        plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(viz_folder/\"training_curve.png\")\n",
    "        plt.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return model, best_val_loss,val_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_validation_metrics(model, validation_pairs, target_length=2000, device=\"cuda\"):\n",
    "    \"\"\"Find the optimal threshold using precision-recall curves.\"\"\"\n",
    "    true_labels = []\n",
    "    distances = []\n",
    "\n",
    "    # Create dataset\n",
    "    val_dataset = ChromaDataset(validation_pairs, target_length=target_length)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, num_workers=0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Computing distances\"):\n",
    "            chroma1 = batch[\"chroma1\"].to(device)\n",
    "            chroma2 = batch[\"chroma2\"].to(device)\n",
    "            labels = batch[\"label\"]\n",
    "\n",
    "            _, _, _, dist = model(chroma1, chroma2)\n",
    "\n",
    "            distances.extend(dist.cpu().numpy())\n",
    "            true_labels.extend(labels.numpy())\n",
    "\n",
    "    # Convert to arrays\n",
    "    distances = np.array(distances).flatten()\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Compute precision-recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(true_labels, 1 - distances)\n",
    "\n",
    "    # Find threshold that maximizes F1 score\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, precision[:-1], label=\"Precision\")\n",
    "    plt.plot(thresholds, recall[:-1], label=\"Recall\")\n",
    "    plt.plot(thresholds, f1_scores[:-1], label=\"F1 Score\")\n",
    "    plt.axvline(x=optimal_threshold, color=\"r\", linestyle=\"--\", label=f\"Optimal Threshold: {optimal_threshold:.3f}\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Precision, Recall, F1 vs Threshold\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(viz_folder /\"threshold_analysis.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return optimal_threshold, f1_scores[optimal_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_distance_distribution(model, validation_pairs, target_length=2000, device=\"cuda\"):\n",
    "    \"\"\"Visualize the distribution of distances to help select a threshold.\"\"\"\n",
    "    same_work_distances = []\n",
    "    diff_work_distances = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for pair in tqdm(validation_pairs, desc=\"Computing distances\"):\n",
    "            file_path1, file_path2, label = pair\n",
    "\n",
    "            # Load and preprocess\n",
    "            dataset = ChromaDataset([(file_path1, file_path2, label)], target_length=target_length)\n",
    "            data = dataset[0]\n",
    "\n",
    "            chroma1 = data[\"chroma1\"].unsqueeze(0).to(device)\n",
    "            chroma2 = data[\"chroma2\"].unsqueeze(0).to(device)\n",
    "\n",
    "            _, _, _, dist = model(chroma1, chroma2)\n",
    "            dist_value = dist.item()\n",
    "\n",
    "            if label == 1:  # Same work\n",
    "                same_work_distances.append(dist_value)\n",
    "            else:  # Different works\n",
    "                diff_work_distances.append(dist_value)\n",
    "\n",
    "    # Visualize distributions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot histograms\n",
    "    plt.hist(same_work_distances, bins=30, alpha=0.5, label=\"Same Work (Covers)\")\n",
    "    plt.hist(diff_work_distances, bins=30, alpha=0.5, label=\"Different Works\")\n",
    "\n",
    "    # Calculate potential threshold using distribution statistics\n",
    "    same_mean = np.mean(same_work_distances)\n",
    "    diff_mean = np.mean(diff_work_distances)\n",
    "    suggested_threshold = (same_mean + diff_mean) / 2\n",
    "\n",
    "    # Add vertical line for suggested threshold\n",
    "    plt.axvline(x=suggested_threshold, color=\"r\", linestyle=\"--\", label=f\"Suggested Threshold: {suggested_threshold:.3f}\")\n",
    "\n",
    "    plt.xlabel(\"Distance\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distribution of Distances on validation set and suggested threshold\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(viz_folder /\"validation_distance_distribution.png\")\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Same work distances: mean={same_mean:.3f}, std={np.std(same_work_distances):.3f}\")\n",
    "    print(f\"Different work distances: mean={diff_mean:.3f}, std={np.std(diff_work_distances):.3f}\")\n",
    "    print(f\"Suggested threshold: {suggested_threshold:.3f}\")\n",
    "\n",
    "    return suggested_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cover_relationship(model, file_path1, file_path2, target_length=2000, threshold=0.5, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Predict if two songs are covers of each other.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Siamese network\n",
    "        file_path1: Path to first H5 file\n",
    "        file_path2: Path to second H5 file\n",
    "        target_length: Target sequence length\n",
    "        threshold: Distance threshold for classification\n",
    "        device: Device to run prediction on\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "\n",
    "    # Load model if path is provided\n",
    "    if isinstance(model, str):\n",
    "        model_state = torch.load(model, map_location=device)\n",
    "        model = SiameseNet()\n",
    "        model.load_state_dict(model_state[\"model_state_dict\"])\n",
    "        model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Create a temporary dataset and load the features\n",
    "    dataset = ChromaDataset([(file_path1, file_path2, 0)], target_length=target_length)\n",
    "    data = dataset[0]\n",
    "\n",
    "    chroma1 = data[\"chroma1\"].unsqueeze(0).to(device)\n",
    "    chroma2 = data[\"chroma2\"].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        similarity, _, _, distance = model(chroma1, chroma2)\n",
    "\n",
    "    distance_value = distance.item()\n",
    "    is_cover = distance_value < threshold\n",
    "\n",
    "    result = {\n",
    "        \"is_cover\": is_cover,\n",
    "        \"distance\": distance_value,\n",
    "        \"similarity\": similarity.item(),\n",
    "        \"confidence\": 1 - (distance_value / 2.0) if is_cover else distance_value / 2.0,\n",
    "        \"explanation\": get_comparison_explanation(distance_value, threshold),\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_comparison_explanation(distance, threshold):\n",
    "    \"\"\"\n",
    "    Generate a human-readable explanation of the comparison result.\n",
    "\n",
    "    Args:\n",
    "        distance: Computed distance between songs\n",
    "        threshold: Threshold for classification\n",
    "\n",
    "    Returns:\n",
    "        Explanation string\n",
    "    \"\"\"\n",
    "    if distance < threshold:\n",
    "        distance_level = \"very similar\" if distance < threshold / 2 else \"similar\"\n",
    "        return f\"The performances appear to be covers ({distance_level}). The distance value is {distance:.3f}, below the threshold of {threshold:.3f}.\"\n",
    "    else:\n",
    "        return f\"The performances do not appear to be covers. The distance ({distance:.3f}) exceeds the threshold ({threshold:.3f}).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pairs...\n",
      "Generating positive pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 1914/3000 [00:00<00:00, 3812.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating negative pairs...\n",
      "Generated 100 negative pairs\n",
      "Generated 200 negative pairs\n",
      "Generated 300 negative pairs\n",
      "Generated 400 negative pairs\n",
      "Generated 500 negative pairs\n",
      "Generated 600 negative pairs\n",
      "Generated 700 negative pairs\n",
      "Generated 800 negative pairs\n",
      "Generated 900 negative pairs\n",
      "Generated 1000 negative pairs\n",
      "Generated 1100 negative pairs\n",
      "Generated 1200 negative pairs\n",
      "Generated 1300 negative pairs\n",
      "Generated 1400 negative pairs\n",
      "Generated 1500 negative pairs\n",
      "Generated 1600 negative pairs\n",
      "Generated 1700 negative pairs\n",
      "Generated 1800 negative pairs\n",
      "Generated 1900 negative pairs\n",
      "Generated 2000 negative pairs\n",
      "Generated 2100 negative pairs\n",
      "Generated 2200 negative pairs\n",
      "Generated 2300 negative pairs\n",
      "Generated 2400 negative pairs\n",
      "Generated 2500 negative pairs\n",
      "Generated 2600 negative pairs\n",
      "Generated 2700 negative pairs\n",
      "Generated 2800 negative pairs\n",
      "Generated 2900 negative pairs\n",
      "Generated 3000 negative pairs\n",
      "Generated 3100 negative pairs\n",
      "Generated 3200 negative pairs\n",
      "Generated 3300 negative pairs\n",
      "Generated 3400 negative pairs\n",
      "Generated 3500 negative pairs\n",
      "Generated 3600 negative pairs\n",
      "Generated 3700 negative pairs\n",
      "Generated 3800 negative pairs\n",
      "Generated 3900 negative pairs\n",
      "Generated 4000 negative pairs\n",
      "Training pairs: 6400\n",
      "Validation pairs: 1600\n",
      "Using device: cuda\n",
      "\n",
      "==================== Epoch 1/10 ====================\n",
      "Starting training phase...\n",
      "GPU memory at start: 0.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 7/25 [01:51<04:49, 16.11s/it]"
     ]
    }
   ],
   "source": [
    "base_folder = r\"D:\\TACOS\\da-tacos_benchmark_subset_crema\\da-tacos_benchmark_subset_crema\"\n",
    "model_save_path = \"siamese_cover_detector.pth\"\n",
    "\n",
    "# Train the model\n",
    "model, best_val_loss,val_pairs = train_model(\n",
    "    base_folder=base_folder,\n",
    "    model_save_path=model_save_path,\n",
    "    num_epochs=10, #5\n",
    "    batch_size=256, #64\n",
    "    learning_rate=0.001,\n",
    "    embedding_size=512, #128\n",
    "    margin=1.0,\n",
    "    target_length=10000, #1000\n",
    "    num_positive=4000, #1000\n",
    "    num_negative=4000, #1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, determine the optimal threshold using your validation data\n",
    "validation_pairs = val_pairs # List of tuples (file_path1, file_path2, label)\n",
    "suggested_threshold_1, f1_score = analyse_validation_metrics(model, validation_pairs)\n",
    "print(f\"Validation metrics analysis threshold: {suggested_threshold_1:.3f}\")\n",
    "\n",
    "# Alternatively, analyze the distance distribution\n",
    "suggested_threshold_2 = analyze_distance_distribution(model, validation_pairs)\n",
    "print(f\"Distance distribution analysis threshold: {suggested_threshold_2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_threshold = (suggested_threshold_1 + suggested_threshold_2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(suggested_threshold_1,suggested_threshold_2,optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf1_path = r\"D:\\TACOS\\da-tacos_benchmark_subset_crema\\da-tacos_benchmark_subset_crema\\W_4726_crema\\P_34522_crema.h5\"\n",
    "perf2_path = r\"D:\\TACOS\\da-tacos_benchmark_subset_crema\\da-tacos_benchmark_subset_crema\\W_4726_crema\\P_788651_crema.h5\"\n",
    "result = predict_cover_relationship(model=model_save_path, file_path1=perf1_path, file_path2=perf2_path, threshold=optimal_threshold)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf1_path = r\"D:\\TACOS\\da-tacos_benchmark_subset_crema\\da-tacos_benchmark_subset_crema\\W_129526_crema\\P_247734_crema.h5\"\n",
    "perf2_path = r\"D:\\TACOS\\da-tacos_benchmark_subset_crema\\da-tacos_benchmark_subset_crema\\W_25511_crema\\P_281569_crema.h5\"\n",
    "result = predict_cover_relationship(model=model_save_path, file_path1=perf1_path, file_path2=perf2_path, threshold=optimal_threshold)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
