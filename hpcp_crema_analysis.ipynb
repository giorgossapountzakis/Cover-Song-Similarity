{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import math\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from bitsandbytes.optim import Adam8bit\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "from pypdf import PdfReader\n",
    "import json\n",
    "from toolz import compose\n",
    "\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class SongFeatureExplorer:\n",
    "    def __init__(self, song_folder_path):\n",
    "        self.song_path = Path(song_folder_path)\n",
    "        self.h5_files = sorted([f for f in os.listdir(song_folder_path) if f.endswith('.h5')])\n",
    "        \n",
    "    def explore_file_structure(self):\n",
    "        \"\"\"Print the structure of first h5 file to understand its organization\"\"\"\n",
    "        if self.h5_files:\n",
    "            first_file = self.song_path / self.h5_files[0]\n",
    "            with h5py.File(first_file, 'r') as f:\n",
    "                print(\"\\nExploring structure of:\", first_file.name)\n",
    "                self._print_structure(f)\n",
    "    \n",
    "    def _print_structure(self, obj, level=0):\n",
    "        \"\"\"Recursively print h5 file structure with proper indentation\"\"\"\n",
    "        indent = \"  \" * level\n",
    "        for key in obj.keys():\n",
    "            if isinstance(obj[key], h5py.Dataset):\n",
    "                print(f\"{indent}Dataset: {key}, Shape: {obj[key].shape}\")\n",
    "            else:\n",
    "                print(f\"{indent}Group: {key}\")\n",
    "                self._print_structure(obj[key], level + 1)\n",
    "    \n",
    "    def load_all_covers(self, feature_type='hpcp'):\n",
    "        \"\"\"Load feature data from all covers, with improved error handling\"\"\"\n",
    "        covers = []\n",
    "        for h5_file in self.h5_files:\n",
    "            try:\n",
    "                file_path = self.song_path / h5_file\n",
    "                with h5py.File(file_path, 'r') as f:\n",
    "                    # First, let's see what we have in the file\n",
    "                    if len(f.keys()) == 0:\n",
    "                        print(f\"Warning: No data in {h5_file}\")\n",
    "                        continue\n",
    "                        \n",
    "                    # Get the data based on file structure\n",
    "                    if feature_type in f:  # If feature is at root level\n",
    "                        feature_data = f[feature_type][:]\n",
    "                        covers.append(feature_data)\n",
    "                    else:  # Try to find feature in first group\n",
    "                        first_group = list(f.keys())[0]\n",
    "                        if feature_type in f[first_group]:\n",
    "                            feature_data = f[first_group][feature_type][:]\n",
    "                            covers.append(feature_data)\n",
    "                        else:\n",
    "                            print(f\"Warning: {feature_type} not found in {h5_file}\")\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {h5_file}: {str(e)}\")\n",
    "                \n",
    "        return covers\n",
    "    \n",
    "    def plot_all_covers(self, feature_type='hpcp'):\n",
    "        \"\"\"Plot all covers with improved visualization and error handling\"\"\"\n",
    "        covers = self.load_all_covers(feature_type)\n",
    "        \n",
    "        if not covers:\n",
    "            print(f\"No {feature_type} features found in any of the files\")\n",
    "            return\n",
    "            \n",
    "        # Create figure with subplots\n",
    "        n_covers = len(covers)\n",
    "        fig, axes = plt.subplots(n_covers, 1, figsize=(15, 4*n_covers))\n",
    "        if n_covers == 1:\n",
    "            axes = [axes]\n",
    "            \n",
    "        # Plot each cover\n",
    "        for idx, (ax, cover) in enumerate(zip(axes, covers)):\n",
    "            im = ax.imshow(cover.T, aspect='auto', origin='lower',\n",
    "                         interpolation='nearest', cmap='viridis')\n",
    "            ax.set_title(f'cover {idx+1} - Shape: {cover.shape}')\n",
    "            ax.set_ylabel('Pitch Class')\n",
    "            plt.colorbar(im, ax=ax)\n",
    "            \n",
    "        axes[-1].set_xlabel('Time Frame')\n",
    "        plt.suptitle(f'{feature_type.upper()} Features Across All covers', y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics for each cover\n",
    "        print(\"\\ncover Statistics:\")\n",
    "        for idx, cover in enumerate(covers):\n",
    "            print(f\"\\ncover {idx+1}:\")\n",
    "            print(f\"Shape: {cover.shape}\")\n",
    "            print(f\"Mean: {np.mean(cover):.3f}\")\n",
    "            print(f\"Std: {np.std(cover):.3f}\")\n",
    "            print(f\"Range: [{np.min(cover):.3f}, {np.max(cover):.3f}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explorer = SongFeatureExplorer(r'D:\\TACOS\\da-tacos_benchmark_subset_hpcp\\da-tacos_benchmark_subset_hpcp\\W_18_hpcp')\n",
    "explorer.explore_file_structure()\n",
    "explorer.plot_all_covers('hpcp') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explorer = SongFeatureExplorer(r'D:\\TACOS\\da-tacos_benchmark_subset_crema\\da-tacos_benchmark_subset_crema\\W_18_crema')\n",
    "explorer.explore_file_structure()\n",
    "explorer.plot_all_covers('crema') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from scipy.spatial.distance import cosine\n",
    "from dtaidistance import dtw\n",
    "from scipy import stats\n",
    "\n",
    "class FeatureComparator:\n",
    "    def __init__(self, song_folder_path):\n",
    "        self.song_path = Path(song_folder_path)\n",
    "        self.h5_files = sorted([f for f in os.listdir(song_folder_path) if f.endswith('.h5')])\n",
    "        self.work_id = self.song_path.name.split('_')[1]\n",
    "\n",
    "    def compute_feature_similarity(self, feature_type='crema'):\n",
    "        features = []\n",
    "        perf_ids = []\n",
    "        \n",
    "        # Load all features\n",
    "        for h5_file in self.h5_files:\n",
    "            try:\n",
    "                with h5py.File(self.song_path / h5_file, 'r') as f:\n",
    "                    # Get feature data\n",
    "                    if feature_type in f:\n",
    "                        feature_data = f[feature_type][:]\n",
    "                    else:\n",
    "                        first_key = list(f.keys())[0]\n",
    "                        feature_data = f[first_key][:]\n",
    "                    \n",
    "                    features.append(feature_data)\n",
    "                    perf_ids.append(h5_file.split('.')[0])\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {h5_file}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if len(features) < 2:\n",
    "            print(\"Not enough features loaded for comparison\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nAnalyzing Work ID: {self.work_id}\")\n",
    "        print(f\"Number of performances: {len(features)}\")\n",
    "        \n",
    "        # Compute similarities between all pairs\n",
    "        similarities = []\n",
    "        for i in range(len(features)):\n",
    "            for j in range(i+1, len(features)):\n",
    "                sim = self._compute_similarity_metrics(features[i], features[j])\n",
    "                similarities.append({\n",
    "                    'perf1': perf_ids[i],\n",
    "                    'perf2': perf_ids[j],\n",
    "                    'metrics': sim\n",
    "                })\n",
    "\n",
    "        # Print results\n",
    "        self._print_similarity_results(similarities)\n",
    "        \n",
    "    def _compute_similarity_metrics(self, feat1, feat2):\n",
    "        \"\"\"\n",
    "        Compute multiple similarity metrics between two feature sequences,\n",
    "        handling different sequence lengths.\n",
    "        \"\"\"\n",
    "        # Get global statistics for each sequence\n",
    "        stats1 = self._compute_sequence_stats(feat1)\n",
    "        stats2 = self._compute_sequence_stats(feat2)\n",
    "        \n",
    "        # Compute statistical similarity\n",
    "        stat_sim = self._compare_statistics(stats1, stats2)\n",
    "        \n",
    "        # Compute frame-level similarities using sequence averaging\n",
    "        frame_sim = self._compute_frame_similarities(feat1, feat2)\n",
    "        \n",
    "        return {**stat_sim, **frame_sim}\n",
    "    \n",
    "    def _compute_sequence_stats(self, feat):\n",
    "        \"\"\"\n",
    "        Compute statistical properties of the feature sequence\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'mean': np.mean(feat),\n",
    "            'std': np.std(feat),\n",
    "            'max': np.max(feat),\n",
    "            'min': np.min(feat),\n",
    "            'median': np.median(feat),\n",
    "            'q1': np.percentile(feat, 25),\n",
    "            'q3': np.percentile(feat, 75)\n",
    "        }\n",
    "    \n",
    "    def _compare_statistics(self, stats1, stats2):\n",
    "        \"\"\"\n",
    "        Compare statistical properties between sequences\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'mean_diff': abs(stats1['mean'] - stats2['mean']),\n",
    "            'std_ratio': stats1['std'] / stats2['std'] if stats2['std'] != 0 else float('inf'),\n",
    "            'range_overlap': min(stats1['max'], stats2['max']) - max(stats1['min'], stats2['min']),\n",
    "            'iqr_ratio': ((stats1['q3'] - stats1['q1']) / \n",
    "                         (stats2['q3'] - stats2['q1']) if (stats2['q3'] - stats2['q1']) != 0 \n",
    "                         else float('inf'))\n",
    "        }\n",
    "    \n",
    "    def _compute_frame_similarities(self, feat1, feat2):\n",
    "        \"\"\"\n",
    "        Compute frame-level similarities between sequences of different lengths\n",
    "        \"\"\"\n",
    "        # Compute average feature vectors\n",
    "        avg1 = np.mean(feat1, axis=0)\n",
    "        avg2 = np.mean(feat2, axis=0)\n",
    "        \n",
    "        # Compute cosine similarity between average vectors\n",
    "        cos_sim = 1 - cosine(avg1, avg2)\n",
    "        \n",
    "        # Compute correlation between average patterns\n",
    "        corr = stats.pearsonr(avg1, avg2)[0]\n",
    "        \n",
    "        # Compute normalized Euclidean distance\n",
    "        eucl_dist = np.linalg.norm(avg1 - avg2) / np.sqrt(len(avg1))\n",
    "        \n",
    "        return {\n",
    "            'cosine_similarity': cos_sim,\n",
    "            'correlation': corr,\n",
    "            'euclidean_distance': eucl_dist\n",
    "        }\n",
    "    \n",
    "    def _print_similarity_results(self, similarities):\n",
    "        \"\"\"\n",
    "        Print similarity analysis results in a structured format\n",
    "        \"\"\"\n",
    "        print(\"\\nPairwise Similarity Analysis:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Find most and least similar pairs\n",
    "        most_similar = max(similarities, key=lambda x: x['metrics']['cosine_similarity'])\n",
    "        least_similar = min(similarities, key=lambda x: x['metrics']['cosine_similarity'])\n",
    "        \n",
    "        for sim in similarities:\n",
    "            print(f\"\\nComparing {sim['perf1']} vs {sim['perf2']}:\")\n",
    "            print(f\"Cosine Similarity: {sim['metrics']['cosine_similarity']:.3f}\")\n",
    "            print(f\"Correlation: {sim['metrics']['correlation']:.3f}\")\n",
    "            print(f\"Euclidean Distance: {sim['metrics']['euclidean_distance']:.3f}\")\n",
    "            print(f\"Statistical Differences:\")\n",
    "            print(f\"  Mean Difference: {sim['metrics']['mean_diff']:.3f}\")\n",
    "            print(f\"  Std Ratio: {sim['metrics']['std_ratio']:.3f}\")\n",
    "            print(f\"  Range Overlap: {sim['metrics']['range_overlap']:.3f}\")\n",
    "        \n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"Most similar pair: {most_similar['perf1']} - {most_similar['perf2']}\")\n",
    "        print(f\"Least similar pair: {least_similar['perf1']} - {least_similar['perf2']}\")\n",
    "\n",
    "# Example usage:\n",
    "# comparator = FeatureComparator('path_to_song_folder')\n",
    "# comparator.compute_feature_similarity('crema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparator = FeatureComparator(r'D:\\TACOS\\da-tacos_benchmark_subset_hpcp\\da-tacos_benchmark_subset_hpcp\\W_18_hpcp')\n",
    "comparator.compute_feature_similarity('hpcp')  # or 'hpcp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparator = FeatureComparator(r'D:\\TACOS\\da-tacos_benchmark_subset_crema\\da-tacos_benchmark_subset_crema\\W_18_crema')\n",
    "comparator.compute_feature_similarity('crema')  # or 'crema'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepdish as dd\n",
    "\n",
    "file_path = r'D:\\TACOS\\da-tacos_benchmark_subset_hpcp\\da-tacos_benchmark_subset_hpcp\\W_22_hpcp\\P_22_hpcp.h5'\n",
    "P_22_hpcp = dd.io.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
